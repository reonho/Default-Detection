{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4Rm0wjQMUHi"
   },
   "source": [
    "# BUILDING A DEFUALT DETECTION MODEL\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. Problem Description (Brief Write Up)\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Data Pre-processing\n",
    "4. Model Selection\n",
    "5. Evaluation\n",
    "6. Discussion and Possible Improvements\n",
    "\n",
    "## 1. Problem Description\n",
    "\n",
    "The goal of this project is to predict a binary target feature (default or not) valued 0 (= not default) or 1 (= default). This project will cover the entire data science pipeline, from data analysis to model evaluation. We will be trying several models to predict default status, and choosing the most appropriate one at the end. \n",
    "\n",
    "The data set we will be working on contains payment information of 30,000 credit card holders obtained from a bank in Taiwan, and each data sample is described by 23 feature attributes and the binary target feature (default or not).\n",
    "\n",
    "The 23 explanatory attributes and their explanations (from the data provider) are as follows:\n",
    "\n",
    "### X1 - X5: Indivual attributes of customer\n",
    "\n",
    "X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "\n",
    "X2: Gender (1 = male; 2 = female). \n",
    "\n",
    "X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "\n",
    "X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "\n",
    "X5: Age (year). \n",
    "\n",
    "### X6 - X11: Repayment history from April to Septemeber 2005\n",
    "The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months, . . . 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
    "\n",
    "\n",
    "X6 = the repayment status in September, 2005\n",
    "\n",
    "X7 = the repayment status in August, 2005\n",
    "\n",
    "X8 = the repayment status in July, 2005\n",
    "\n",
    "X9 = the repayment status in June, 2005\n",
    "\n",
    "X10 = the repayment status in May, 2005\n",
    "\n",
    "X11 = the repayment status in April, 2005. \n",
    "\n",
    "### X12 - X17: Amount of bill statement (NT dollar) from April to September 2005\n",
    "\n",
    "X12 = amount of bill statement in September, 2005; \n",
    "\n",
    "X13 = amount of bill statement in August, 2005\n",
    "\n",
    ". . .\n",
    "\n",
    "X17 = amount of bill statement in April, 2005. \n",
    "\n",
    "### X18 - X23: Amount of previous payment (NT dollar)\n",
    "X18 = amount paid in September, 2005\n",
    "\n",
    "X19 = amount paid in August, 2005\n",
    "\n",
    ". . .\n",
    "\n",
    "X23 = amount paid in April, 2005. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM_aIU6UPHe4"
   },
   "source": [
    "## EDA\n",
    "\n",
    "In this section we will explore the data set, its shape and its features to get an idea of the data.\n",
    "\n",
    "### Importing packages and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Is0wEkk3LJCt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_Z7u_9vRC5m"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhmX9KWWyrUW"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/reonho/bt2101disrudy/master/card.csv'\n",
    "df = pd.read_csv(url,  header = 1, index_col = 0)\n",
    "# Dataset is now stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "FhJ2eAxVQhBm",
    "outputId": "7f79bb40-f08f-4709-e7d4-1f747bb8af2f"
   },
   "outputs": [],
   "source": [
    "#rename the target variable to \"Y\" for convenience\n",
    "df[\"Y\"] = df[\"default payment next month\"] \n",
    "df = df.drop(\"default payment next month\", axis = 1)\n",
    "df0 = df #backup of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zcuPyfM86AKj",
    "outputId": "89bb2e37-a3ba-43e5-99a7-6917f24acc3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size = df.shape\n",
    "print(\"Data has {} Columns and {} Rows\".format(size[1], size[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QVaSnvJP3VbO",
    "outputId": "4bf72e64-2d0c-41c3-85b5-3bd6e70920d3"
   },
   "outputs": [],
   "source": [
    "#check for null values\n",
    "df.isnull().any().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVYXnIGH9Zq6"
   },
   "source": [
    "From the above analyses, we observe that:\n",
    "1. The data indeed has 30000 rows and 24 columns\n",
    "2. There are no null values\n",
    "\n",
    "We will now explore the features more in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6hhPNl1Slau"
   },
   "source": [
    "### Exploring the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Sp2F3gzXX2F"
   },
   "source": [
    "**1) Exploring target attribute:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DCSEICWwXWgX",
    "outputId": "9545da56-f31b-48f2-a271-db0e18677beb"
   },
   "outputs": [],
   "source": [
    "All = df.shape[0]\n",
    "default = df[df['Y'] == 1]\n",
    "nondefault = df[df['Y'] == 0]\n",
    "\n",
    "x = len(default)/All\n",
    "y = len(nondefault)/All\n",
    "\n",
    "print('defaults :',x*100,'%')\n",
    "print('non defaults :',y*100,'%')\n",
    "\n",
    "# plotting target attribute against frequency\n",
    "labels = ['non default','default']\n",
    "classes = pd.value_counts(df['Y'], sort = True)\n",
    "classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Target attribute distribution\")\n",
    "plt.xticks(range(2), labels)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tysR0WHw4SGU"
   },
   "source": [
    "**2) Exploring categorical attributes**\n",
    "\n",
    "Categorical attributes are:\n",
    "- Sex\n",
    "- Education\n",
    "- Marriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "s61SSRII00UB",
    "outputId": "69df981f-8c36-43a9-d155-a6553adbba0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df[\"SEX\"].value_counts().apply(lambda r: r/All*100))\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df[\"EDUCATION\"].value_counts().apply(lambda r: r/All*100))\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(df[\"MARRIAGE\"].value_counts().apply(lambda r: r/All*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uudv5XE828nb"
   },
   "source": [
    "**Findings**\n",
    "\n",
    "- Categorical variable SEX does not seem to have any missing/extra groups, and it is separated into Male = 1 and Female = 2\n",
    "- Categorical variable MARRIAGE seems to have unknown group = 0, which could be assumed to be missing data, with other groups being Married = 1, Single = 2, Others = 3\n",
    "- Categorical variable EDUCATION seems to have unknown group = 0,5,6, with other groups being graduate school = 1, university = 2, high school = 3, others = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "U3IJzhwwe5KK",
    "outputId": "cb61e112-a3ec-4a37-c1a0-0ffc9ebcbf89",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#proportion of target attribute (for reference)\n",
    "print('Total target attributes:')\n",
    "print('non defaults :',y*100,'%')\n",
    "print('defaults :',x*100,'%')\n",
    "print(\"--------------------------------------------------------\")\n",
    "#analysing default payment with Sex\n",
    "sex_target = pd.crosstab(df[\"Y\"], df[\"SEX\"]).apply(lambda r: r/r.sum()*100).rename(columns = {1: \"Male\", 2: \"Female\"}, index = {0: \"non defaults\", 1: \"defaults\"})\n",
    "print(sex_target)\n",
    "print(\"--------------------------------------------------------\")\n",
    "#analysing default payment with education\n",
    "education_target = pd.crosstab(df[\"Y\"], df[\"EDUCATION\"]).apply(lambda r: r/r.sum()*100).rename(index = {0: \"non defaults\", 1: \"defaults\"})\n",
    "print(education_target)\n",
    "print(\"--------------------------------------------------------\")\n",
    "#analysing default payment with marriage\n",
    "marriage_target = pd.crosstab(df[\"Y\"], df[\"MARRIAGE\"]).apply(lambda r: r/r.sum()*100).rename(columns = {0: \"unknown\",1: \"married\", 2: \"single\", 3: \"others\"},index = {0: \"non defaults\", 1: \"defaults\"})\n",
    "print(marriage_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kOriUQ0wxbhD"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "From the analyses above we conclude that\n",
    "\n",
    "1. The categorical data is noisy - EDUCATION and MARRIAGE contains unexplained/anomalous data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77GAylGWnPJO"
   },
   "source": [
    "**3) Analysis of Numerical Attributes**\n",
    "\n",
    "The numerical attributes are:\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "HEcCl5Rj-N0T",
    "outputId": "a59f7092-366e-47ec-c67b-e18f02d84ac4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#printing numerical attributes\n",
    "pd.DataFrame(df.drop(['SEX', 'EDUCATION', 'MARRIAGE','Y'], axis = 1).columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['SEX', 'EDUCATION', 'MARRIAGE','Y'], axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of PAY_0 to PAY_6**\n",
    "\n",
    "We observe that the minimum value of PAY_0 to PAY_6 is -2. The dataset's author has explained these factors (PAY_0 to PAY_6) as the number of months of payment delay, that is, 1= payment delay of one month; 2= payment delay of two months and so on. \n",
    "\n",
    "However, the presence of -2, -1 in these columns indicates that\n",
    "1. There is anomalous data, OR \n",
    "2. The numbers do not strictly correspond to the number of months of payment delay. \n",
    "\n",
    "This means we must conduct some data transformation.\n",
    "\n",
    "According to **(link)**, the numeric value in these attributes shows the past history of a credit card holder, where -2 means: No consumption of credit card, -1 means that holder paid the full balance, and 0 means the use of revolving credit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "awXnqvLOS-wB",
    "outputId": "a77b53b8-011e-4f53-b7b7-20d80bbc1777",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_histograms(df, variables, n_rows, n_cols, n_bins):\n",
    "    fig=plt.figure()\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        df[var_name].hist(bins=n_bins,ax=ax)\n",
    "        ax.set_title(var_name)\n",
    "    fig.tight_layout()  # Improves appearance a bit.\n",
    "    plt.show()\n",
    "\n",
    "PAY = df[['PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\n",
    "BILLAMT = df[['BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]\n",
    "PAYAMT = df[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "\n",
    "draw_histograms(PAY, PAY.columns, 2, 3, 10)\n",
    "draw_histograms(BILLAMT, BILLAMT.columns, 2, 3, 10)\n",
    "draw_histograms(PAYAMT, PAYAMT.columns, 2, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6c_Gz6wUrJ8"
   },
   "source": [
    "We observe that the \"repayment status\" attributes are the most highly correlated with the target variable and we would expect them to be more significant in predicting credit default. In fact the later the status (pay_0 is later than pay_6), the more correlated it is.\n",
    "\n",
    "Now that we have an idea of the features, we will move on to feature selection and data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQBksEyEf4Sf"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "It was previously mentioned that our data had a bit of noise, so we will clean up the data in this part. Additionally, we will conduct some feature selection.\n",
    "1. Removing Noise - Inconsistencies\n",
    "2. Dealing with negative values of PAY_0 to PAY_6\n",
    "3. Outliers\n",
    "4. One Hot Encoding\n",
    "5. Train Test Split\n",
    "6. Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Noise\n",
    "First, we found in our data exploration that education has unknown groups 0, 5 and 6. These will be replaced with Education = Others, which has value 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EDUCATION'].replace([0,5,6], 4, regex=True, inplace=True)\n",
    "df[\"EDUCATION\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for Marriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MARRIAGE'].replace([0], 3, regex=True, inplace=True)\n",
    "df[\"MARRIAGE\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating negative and positive values for PAY_0 to PAY_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we are going to extract the negative values of PAY_0 to PAY_6 as another categorical feature. This way, PAY_0 to PAY_6 can be thought of purely as the months of delay of payments.\n",
    "\n",
    "The negative values will form a categorical variable. e.g. negative values of PAY_0 will form the categorical variable S_0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,7):\n",
    "    try:\n",
    "        df[\"S_\" + str(i)] = [x  if x < 1 else 1 for x in df[\"PAY_\" + str(i)]]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dummy variables for negative values')\n",
    "df[[\"S_0\", \"S_2\", \"S_3\", \"S_4\", \"S_5\", \"S_6\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attributes representing positive values\n",
    "for col in [\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]:\n",
    "    df[col].replace([0,-1,-2], 0, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Next, we would like to remove outliers from the continuous variables. Assuming that all the data points are normally distributed, we will consider a point an outlier if it falls outside the 99% interval of a distribution. (Critical value = 2.58) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#we are only concerned with the ordinal data\n",
    "o = pd.DataFrame(df.drop(['Y','EDUCATION', 'MARRIAGE', \"SEX\",\"S_0\", \"S_2\", \"S_3\", \"S_4\", \"S_5\", \"S_6\",\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"], axis=1))\n",
    "#rows where the absolute z score of all columns are less than 2.58 (critical value)\n",
    "rows = (np.abs(stats.zscore(o)) < 2.58).all(axis=1)\n",
    "df = df[rows]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "The models used subsequently may have difficulty converging before the maximum number of iterations allowed\n",
    "is reached if the data is not normalized. Additionaly, Multi-layer Perceptron is sensitive to feature scaling, so we will use StandardScaler for standardization. We only want to scale the numerical factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cols = df.drop(['Y','EDUCATION', 'MARRIAGE', \"SEX\",\"S_0\", \"S_2\", \"S_3\", \"S_4\", \"S_5\", \"S_6\",\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"], axis =1)\n",
    "df1 = df.copy()\n",
    "df1[cols.columns] = scaler.fit_transform(cols)\n",
    "df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding for Categorical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some models, categorical variables which are encoded numerically will be erroneously treated as ordinal data. To understand why this is a problem, consider the \"Education\" column for our dataset.\n",
    "\n",
    "A logistic regression model, for example, will assume that the difference in odds of default between education = 1 and education = 2 is the same as the difference between education = 2 and 3. This is wrong because the difference in odds between a graduate degree and university (1 and 2) is likely to be different from that between univeristy education and high school education (2 and 3).\n",
    "\n",
    "One hot encoding will allow our models to treat these columns explicitly as categorical features.\n",
    "\n",
    "The following categorical columns will be one-hot encoded\n",
    "\n",
    "1. EDUCATION\n",
    "2. MARRIAGE\n",
    "3. S0 - S6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onenc = OneHotEncoder(categories='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for EDUCATION and MARRIAGE\n",
    "onehot = pd.DataFrame(onenc.fit_transform(df[['EDUCATION', 'MARRIAGE']]).toarray())\n",
    "onehot.columns= names = [\"GRAD\",\"UNI\",\"HS\",\"OTHER-EDU\",\"MARRIED\",\"SINGLE\",\"OTHER_MS\"]\n",
    "#drop one of each category to prevent dummy variable trap\n",
    "onehot = onehot.drop([\"OTHER-EDU\", \"OTHER_MS\"], axis = 1)\n",
    "onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#one hot encoding for S_0 to S_6\n",
    "onehot_PAY = pd.DataFrame(onenc.fit_transform(df[['S_0', 'S_2', 'S_3', 'S_4', 'S_5', 'S_6']]).toarray())\n",
    "onehot_PAY.columns= onenc.fit(df[[\"S_0\", \"S_2\", \"S_3\", \"S_4\", \"S_5\", \"S_6\"]]).get_feature_names()\n",
    "#drop one of each category to prevent dummy variable trap\n",
    "#onehot = onehot.drop([\"OTHER-EDU\", \"OTHER_MS\"], axis = 1)\n",
    "names = []\n",
    "for X in range(0,7):\n",
    "    if X == 1:\n",
    "        continue\n",
    "    names.append(\"PAY_\"+str(X)+\"_No_Transactions\")\n",
    "    names.append(\"PAY_\"+str(X)+\"_Pay_Duly\")\n",
    "    names.append(\"PAY_\"+str(X)+\"_Revolving_Credit\")\n",
    "    try:\n",
    "        onehot_PAY = onehot_PAY.drop(\"x\" + str(X) +\"_1\", axis =1)\n",
    "    except:\n",
    "        onehot_PAY = onehot_PAY.drop(\"x1_1\", axis =1)\n",
    "onehot_PAY.columns = names\n",
    "onehot_PAY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['EDUCATION', 'MARRIAGE','S_0', 'S_2', 'S_3', 'S_4', 'S_5', 'S_6'], axis = 1)\n",
    "df1 = pd.concat([df1.reset_index(drop=True), onehot], axis=1)\n",
    "df1 = pd.concat([df1.reset_index(drop=True), onehot_PAY], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for perfect collinearity\n",
    "corr = df1.corr()\n",
    "for i in range(len(corr)):\n",
    "    corr.iloc[i,i] = 0\n",
    "#corr[corr == 1] = 0\n",
    "corr[corr.eq(1).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = df1.shape\n",
    "print(\"Data has {} Columns and {} Rows\".format(size[1], size[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Before we conduct feature selection and model selection, we split the data using a train test split according to the project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOB68z_hM1jW"
   },
   "outputs": [],
   "source": [
    "#using holdout sampling for train test split\n",
    "ft = df1.drop(\"Y\", axis = 1)\n",
    "target = df1[\"Y\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(ft,target,test_size=0.33333)\n",
    "#make the results reproducible (according to instructions)\n",
    "np.random.seed(123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter method for feature selection\n",
    "The filter method for feature selection entails selecting relevant attributes before moving on to learning phase.\n",
    "We will utitlise univariate feature selection to reduce the features to the fewer more significant attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest( score_func = chi2, k=10)\n",
    "selector.fit(X_train, y_train)\n",
    "np.set_printoptions(precision=10)\n",
    "chi2data = pd.DataFrame(selector.scores_)\n",
    "chi2data[\"pval\"] = 1 - stats.chi2.cdf(chi2data, 43)\n",
    "chi2data.index = X_train.columns\n",
    "\n",
    "print(\"Significant values are:\")\n",
    "print(chi2data[chi2data[\"pval\"] < 0.05])\n",
    "\n",
    "cols = chi2data[chi2data[\"pval\"] < 0.05].index\n",
    "X_train_filter = X_train[cols]\n",
    "X_test_filter = X_test[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbhlIlQzZz7c"
   },
   "source": [
    "## Model Selection\n",
    "\n",
    "In this part, we will fit machine learning models learnt in BT2101 to this classification problem, and pick the model that can produce the best results.\n",
    "\n",
    "We will be attempting to fit the following models:\n",
    "\n",
    "\n",
    "- Decision Tree \n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Neural Network\n",
    "\n",
    "To make things easier, we define a get_roc function that will plot an ROC curve for all the models we evaluate, and a confusion matrix function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc(model, y_test, X_test, name):\n",
    "    try:\n",
    "        fpr = roc_curve(y_test,model.predict_proba(X_test)[:,1])[0]\n",
    "        tpr = roc_curve(y_test,model.predict_proba(X_test)[:,1])[1]\n",
    "        thresholds = roc_curve(y_test,model.predict_proba(X_test)[:,1])[2]\n",
    "    except:\n",
    "        fpr = roc_curve(y_test,model.predict(X_test))[0]\n",
    "        tpr = roc_curve(y_test,model.predict(X_test))[1]\n",
    "        thresholds = roc_curve(y_test,model.predict(X_test))[2]\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for ' + name)\n",
    "    plt.plot(fpr,tpr,label='ROC curve (AUC = %0.2f)' % (auc(fpr, tpr)))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    #find- best threshold\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    print(\"Optimal Threshold: \" + str(optimal_threshold))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confusion(y_test, predictions, name):\n",
    "    conf = pd.crosstab(y_test,predictions, rownames=['Actual'], colnames=['Predicted'])\n",
    "    print(\"Of \" + str(conf[0][1] + conf[1][1]) + \" Defaulters, the \" + name + \" identified \" + str(conf[1][1])) \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "We will select the model based on the model evaluation. The key metrics we will compute are:\n",
    "\n",
    "1. Accuracy\n",
    "2. Recall\n",
    "3. AUROC\n",
    "\n",
    "Because of the nature of a default detection problem, we would like to prioritise **recall** for defaults. \n",
    "This means we will place more importance in correctly identifying a defaulter than avoiding misclassifying a non-defaulter. (Assumming that the bank loses more money when lending to a defaulter than not lending to a non-defaulter)\n",
    "\n",
    "However, simply predicting every data point as a defaulter will give us 100% recall. We have to also consider accuracy and AUROC to get a better idea of how our model performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(columns=['Model', 'Recall-1', 'AUROC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H89tM6NvaN17"
   },
   "source": [
    "###  Decision Trees\n",
    "\n",
    "#### Theory:\n",
    "The decision tree algorithm aims to recursively split the data points in the training set until the data points are completely separated or well separated. At each iteration, the tree splits the datasets by the feature(s) that give the maximum reduction in heterogeneity, which is calculated by a heterogeneity index.\n",
    "\n",
    "Below is a binary decision tree that has been split for a few iterations.\n",
    "\n",
    "![image.png](https://elf11.github.io/images/decisionTree.png)\n",
    "\n",
    "Since the target for this project is binary (fraud = yes or no) we will be building a binary decision tree, using the the GINI Index as the Heterogeneity index. The GINI is given by:\n",
    "\n",
    "![image.png](https://miro.medium.com/max/664/1*otdoiyIwxJI-UV0ukkyutw.png)\n",
    "\n",
    "The GINI index measures how heterogenous a single node is (0 being completely homogenous and 1 being heterogenous). For each possible split, we will calculate the *weighted sum* of the GINI indices of the child nodes, and choose the split that results in the maximum information gain. i.e. reduction in the weighted sum of the GINI Index.\n",
    "\n",
    "#### Training\n",
    "We will now construct a simple decision tree using the GINI index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, tree.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set accuracy is 1, which means the datapoints are completely separated by the decision tree. We evaluate on the test set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test, tree.predict(X_test), \"Decision Tree (GINI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = get_roc(tree, y_test, X_test, \"Decision Tree (GINI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc[0] = ([\"Decision Tree\" , \n",
    "                      classification_report(y_test, tree.predict(X_test), output_dict = True)[\"1\"][\"recall\"],\n",
    "                      auroc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "#### Theory\n",
    "Random Forest is an ensemble method for the decision tree algorithm. It works by randomly choosing different features and data points to train multiple trees (that is, to form a forest) - and the resulting prediction is decided by the votes from all the trees. \n",
    "\n",
    "Decision Trees are prone to overfitting on the training data, which reduces the performance on the test set. Random Forest mitigates this by training multiple trees. Random Forest is a form of bagging ensemble where the trees are trained concurrently. \n",
    "\n",
    "#### Training\n",
    "To keep things consistent, our Random Forest classifier will also use the GINI Coefficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randf = RandomForestClassifier(n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, randf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set has also been 100% correctly classified by the random forest model. Evaluating with the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test, randf.predict(X_test), \"Decision Tree (Random Forest)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = get_roc(randf, y_test, X_test, \"Decision Tree (Random Forest)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, randf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc[1] = ([\"Random Forest\" , \n",
    "                      classification_report(y_test, randf.predict(X_test), output_dict = True)[\"1\"][\"recall\"],\n",
    "                      auroc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest ensemble performs much better than the decision tree alone. The accuracy and AUROC are both superior to the decision tree alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees Classifier\n",
    "\n",
    "#### Theory\n",
    "In this part we train a gradient boosted trees classifier using xgBoost. xgBoost is short for \"Extreme Gradient Boosting\". It is a boosting ensemble method for decision trees, which means that the trees are trained consecutively, where each new tree added is trained to correct the error from the previous tree.\n",
    "\n",
    "xgBoost uses the gradient descent algorithm that we learnt in BT2101 at each iteration to maximise the reduction in the error term. (More details? math?)\n",
    " \n",
    "#### Training\n",
    "For consistency our xgBoost ensemble will use n_estimators = 300 as we have done for the random forest ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "xgb = GradientBoostingClassifier(n_estimators=300, max_depth = 4)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, xgb.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the xgBoost ensemble did not fully separate the data in the training set. (The default maximum depth is 3, so that might be a factor). Evaluating on the test set,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test, xgb.predict(X_test), \"Decision Tree (Gradient Boosted Trees)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = get_roc(xgb, y_test, X_test, \"Decision Tree (XGBoost)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc[2] = ([\"Gradient Boosted\" , \n",
    "                      classification_report(y_test, xgb.predict(X_test), output_dict = True)[\"1\"][\"recall\"],\n",
    "                      auroc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the accuracy and AUROC, we observe that the XGBoost performs similarly to the random forest ensemble. It has a slight bump in AUROC at 0.76, but the accuracy is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "#### Theory\n",
    "Logistic regression is a regression technnique used to predict binary target variables. It works on the same principles as a linear regression model. \n",
    "\n",
    "Our binary target (default vs non-default) can be expressed in terms of odds of defaulting, which is the ratio of the probability of default and probability of non-default. \n",
    "\n",
    "In the logistic regression model, we log the odds (log-odds) and equate it to a weighted sum of regressors.\n",
    "\n",
    "![image.png](https://wikimedia.org/api/rest_v1/media/math/render/svg/4a5e86f014eb1f0744e280eb0d68485cb8c0a6c3)\n",
    "\n",
    "We then find weights for the regressors that best fits the data. Since the binary target (default or not) follows a bernoulli distribution, each data point has the following probability distribution function:\n",
    "\n",
    "![image.png](https://wikimedia.org/api/rest_v1/media/math/render/svg/614e0c64d59f0ff2e926deafcb2de6e502394fac)\n",
    "\n",
    "We would like to update p for each data point such that the log product (joint probability) of the above function for all data points is maximised. In other words, we are maximising the log-likelihood function.\n",
    "\n",
    "The logistic regression equation produces a \"squashed\" curve like the one below. We then pick a cutoff value for the y axis to classify a data point as 0 (non-default) or 1 (default).\n",
    "\n",
    "![image.png](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1280px-Logistic-curve.svg.png)\n",
    "\n",
    "\n",
    "#### Training\n",
    "We will adopt a top-down approach for training our logistic regression model, i.e. include all regressors first and then remove the most insignificant ones at each iteration to achieve the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = sm.Logit(y_train,X_train).fit()\n",
    "glm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,list(glm.predict(X_train)>0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,list(glm.predict(X_test)>0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logisitc model with all features performs quite well on both the train and test set with an accuracy of about 0.8. We will now try removing all the insignificant features to see how that affects the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all insignificant attributes\n",
    "sig = glm.pvalues[glm.pvalues < 0.05]\n",
    "glm_2 = sm.Logit(y_train,X_train[sig.index]).fit()\n",
    "glm_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,list(glm_2.predict(X_train[sig.index])>0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,list(glm_2.predict(X_test[sig.index])>0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is not much change to the model performance on both the train and test set when we reduce the features, we will use the reduced logistic regression model from this point onwards (Principle of Parsimony). \n",
    "\n",
    "We now Calculate the AUROC for the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_roc(glm_2, y_train, X_train[sig.index], \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the optimal cut off was found to be 0.2697615225249289 using the training data, we will use that as our cut off for our evaluation of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,list(glm_2.predict(X_test[sig.index])>0.2697615225249289)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the training accuracy has gone down when we used the optimal cutoff. However, accuracy may be misleading in a dataset like ours where most of the targets are non-defaults. \n",
    "\n",
    "The recall here is more important - detecting defaulters is more useful than detecting non-defaulters. With a higher recall, our model with lower cutoff is able to correctly catch more defaulters.\n",
    "\n",
    "\n",
    "Calculate the confusion matrices for both cut offs to better compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test,glm_2.predict(X_test[sig.index])>0.2697615225249289, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test,glm_2.predict(X_test[sig.index])>0.50, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the lower cutoff is better able to detect defualts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = get_roc(glm_2, y_test, X_test[sig.index], \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc[3] = [\"Logistic Regression\" , classification_report(y_test,list(glm_2.predict(X_test[sig.index])>0.2697615225249289), output_dict = True)[\"1\"][\"recall\"],auroc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCxBcin11EI8"
   },
   "source": [
    "### Support Vector Machine\n",
    "#### Theory\n",
    "Support vector machines attempt to find an optimal hyperplane that is able to separate the two classes in n-dimensional space. This is done by finding the hyperplane that maximises the distance between itself and support vectors (data points that lie closest to the decision boundary).\n",
    "\n",
    "SVM is computationally expensive for a dataset with a lot of features. Therefore, it is neccessary at this stage to do some data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "We would like to reduce the dimensionality of our dataset before training an SVM on it. This can be done through Principle Component Analysis (PCA). The idea would be to reduce the number of features without loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Component PCA\n",
    "First, we will visualize the information retained after performing a 2 component pca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform pca\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents =  pca.fit_transform(X_train)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount of information each principal component holds after projecting the data to a lower dimensional subspace.\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the information of principal component 1 retained is 28.4% and principal component 2 retained is 17.8%, both of which is quite low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing pca\n",
    "pca_visualize_df = pd.concat([principalDf, y_train], axis = 1)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [1,0]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = pca_visualize_df['Y'] == target\n",
    "    ax.scatter(pca_visualize_df.loc[indicesToKeep, 'principal component 1']\n",
    "               , pca_visualize_df.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is no clear linear separation for the target attributes for 2 pca components, justifying the above percentages. Nonetherless, we will continue to use PCA by finding the  optimmum number of PC components which retains 90% of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA to retain 90% of information\n",
    "perform PCA to reduce components so we can run SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting pca threshold to 90%\n",
    "pca = PCA(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of components after pca\n",
    "print('No. of components before pca: {}'.format(len(X_train.columns)))\n",
    "print('No. of components after pca: {}'.format(pca.n_components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of components is reduced from 26 components to 13 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform pca on training and test attributes\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SVM model\n",
    "Next, we will used the reduced attributes train set to train our SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train our SVM model without parameter tuning\n",
    "nor pca reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "#train svm model without standardization and no parameter tuning\n",
    "clf_original = svm.SVC(kernel = 'rbf', probability = True, gamma = 'scale')\n",
    "clf_original.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc for svm\n",
    "get_roc(clf_original, y_test, X_test, \"SVM original data RBF kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "confusion(y_test,clf_original.predict(X_test), \"SVM original data RBF kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf_original.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, SVM model fit with no tuning or feature reduction with RBF kernal shows low performance. Now, we will fit SVM model with reduced standardized features and access accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that the default values of gamma = 1/13 and c= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train svm model with feature reduction and no parameter tuning\n",
    "clf_reduced = svm.SVC(kernel = 'rbf', probability = True, gamma = 1/13, C = 1)\n",
    "clf_reduced.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc for svm\n",
    "get_roc(clf_reduced, y_test, X_test_pca, \n",
    "        \"SVM reduced features no tuning RBF kernal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "confusion(y_test,clf_reduced.predict(X_test_pca), \"SVM reduced features no tuning RBF kernal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf_reduced.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by reducing features through pca, Although the AUROC did not change much (0.001 increase), the number of correctly idendtified defaulters increased from 297 to 307, suggesting a better recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to find best parameters for SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.01, 0.1, 1,10]\n",
    "    gammas = [0.01, 0.1, 10]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "svc_param_selection(X_train_pca, y_train,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2 folds, it can be found that C = 10, and gamma = 0.01 will have the best svm model with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train svm model with feature reduction and cost = 10, gamma = 0.01\n",
    "clf_reduced_tuned = svm.SVC(kernel = 'rbf', probability = True, C= 10, gamma = 0.1)\n",
    "clf_reduced_tuned.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = get_roc(clf_reduced_tuned, y_test, X_test_pca, \n",
    "        \"SVM reduced features and tuning RBF kernal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "confusion(y_test,clf_reduced_tuned.predict(X_test_pca), \"SVM reduced features and tuning RBF kernal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf_reduced_tuned.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can conclude that fitting SVM model with PCA-reduced features but no parameter tuning is most accurate based on Recall value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, fitting an SVM with PCA-reduced features with default gamma = 1/13 and C = 1 with kernal = 'rbf' is the best model. However, this is only for rbf kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc[4] = ([\"SVM\" , \n",
    "                      classification_report(y_test, clf_reduced_tuned.predict(X_test), output_dict = True)[\"1\"][\"recall\"],\n",
    "                      auroc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "We will now use the train and test sets as defined above and attempt to implement a neural network model on the data\n",
    "\n",
    "#### Theory\n",
    "A neural network is comprised of many layers of perceptrons that take in a vector as input and outputs a value. The outputs from one layer of perceptrons are passed into the next layer of perceptrons as input, until we reach the output layer. Each perceptron combines its input via an activation function. \n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "![image.png](https://www.researchgate.net/profile/Leslaw_Plonka/publication/260080460/figure/fig1/AS:340931325775876@1458295770470/A-simple-neural-network-diagram.png)\n",
    "\n",
    "\n",
    "The network is at first randomly initialised with random weights on all its layers. Training samples are then passed into the network and predictions are made. The training error (difference between the actual value and the predicted value) is used to recalibrate the neural network by changing the weights. The change in weights is found via gradient descent, and  then backpropogated through the neural network to update all layers.\n",
    "\n",
    "\n",
    "This process is repeated iteratively until the model converges (i.e. it cannot be improved further).\n",
    "\n",
    "#### Training\n",
    "Here we create an instance of our model, with 5 layers of 26 neurons each, identical to that of our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(26,26,26,26,26), activation = \"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test,predictions,\"Neural Network (5x26)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = get_roc(mlp, y_test, X_test, \"Neural Network (5x26)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.loc[5] = ([\"Neural Network\" , \n",
    "                      classification_report(y_test, mlp.predict(X_test), output_dict = True)[\"1\"][\"recall\"],\n",
    "                      auroc])\n",
    "\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "#### Theory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=26, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model\n",
    "#recall, accuracy = model.evaluate(df1, target)\n",
    "#print('Accuracy: %.2f' % (accuracy*100))\n",
    "#print('Recall: %.2f' % (recall*100))\n",
    "\n",
    "predictions = list(model.predict(X_test).ravel() > 0.5)\n",
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BT2101 disrudy ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
